

# ğŸ› ï¸ Full Walkthrough: From Scraping Tools to Publishing an Auto README

---

## âœ… Step 1: Install Requirements

### ğŸ“Œ Install Python (if you donâ€™t have it)

* Download from: [https://www.python.org/downloads](https://www.python.org/downloads)
* During install, **check the box** that says **â€œAdd Python to PATHâ€**

### ğŸ“¦ Install required Python packages

#### Copy and paste this

```bash
pip install -r requirements.txt
```

---

## âœ… Step 2: Set Up the Project Folder

```bash
cd arsenal-project
```

---

## âœ… Step 3: Recommended Folder Structure

```
arsenal-project/
â”œâ”€â”€ Data/
â”‚   â””â”€â”€ Canada/
â”œâ”€â”€ CanadaIndiv/
â”œâ”€â”€ CanadaCleaned/
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ Canada/
â”‚       â””â”€â”€ 2023/
â”œâ”€â”€ Links/
â”‚   â”œâ”€â”€ Canada.txt
â”‚   â””â”€â”€ Europe.txt
â”œâ”€â”€ NewLinks/
â”‚   â””â”€â”€ Canada.txt
â”œâ”€â”€ AutoReadme.py         â† final readme generator
â”œâ”€â”€ CategoryPredictor.py  â† LLM-based track classifier
â”œâ”€â”€ run.py                â† one-click execution script
â”œâ”€â”€ README.md             â† auto-generated
```

---

## âœ… Step 4: Sample Input Links

### ğŸ”¹ For modern schedule scraper (`scrape_blackhat_schedule.py`)

**NewLinks/Canada.txt**

```
https://www.blackhat.com/can-24/arsenal/schedule/index.html
https://www.blackhat.com/us-23/arsenal/schedule/index.html
```

### ğŸ”¹ For legacy HTML scraper (`scrape_old_html_schedule.py`)

**Links/Europe.txt**

```
https://www.blackhat.com/asia-18/arsenal.html
```

---

## âœ… Step-by-Step Manual Commands

### ğŸ”¹ 1. Scrape Event Pages

Modern (JS-based):

```bash
python scrape_blackhat_schedule.py
```

Legacy (static HTML):

```bash
python scrape_old_html_schedule.py
```

---

### ğŸ”¹ 2. Add Year and Country to Each Tool

```bash
python update_metadata_fields.py
```

---

### ğŸ”¹ 3. Split All Tools into Individual Files

```bash
python split_tools_to_individual_files.py
```

---

### ğŸ”¹ 4. Predict Tool Categories (Tracks)

```bash
python CategoryPredictor.py
```

> Uses GPT or Gemini to infer category from description
> âœ… Adds: `"Tracks": ["Track: Red Teaming"]`

---

### ğŸ”¹ 5. Add GitHub URLs Using Serper.dev

1. Open `add_github_urls.py` and set your API key:

```python
SERPER_API_KEY = "your-api-key-here"
```

2. Run:

```bash
python add_github_urls.py
```

---

### ğŸ”¹ 6. Flatten Nested Files and add it to the root folder

```bash
python flatten_tool_folders.py
```

---

### ğŸ”¹ 7. Generate README Files

```bash
python AutoReadme.py
```

âœ… Outputs:

* An organized `README.md` at the root
* Sub-readmes under `tools/Region/Year/`

---

## âœ… Optional: One-Click Execution

Use the bundled automation script to run everything in one command:

```bash
python run.py
```

It will:

* Scrape
* Enrich
* Predict
* Add GitHub URLs
* Flatten
* Generate README

---

## ğŸ“¦ Final Output

* `README.md` organized by region â†’ year â†’ category
* `tools/{Region}/{Year}/README.md` (sub-lists)
* One JSON file per tool, enriched and categorized

---

## ğŸ“ Script Summary

| Script                               | Purpose                                |
| ------------------------------------ | -------------------------------------- |
| `scrape_blackhat_schedule.py`        | Scrape modern Black Hat pages          |
| `scrape_old_html_schedule.py`        | Scrape legacy static HTML pages        |
| `update_metadata_fields.py`          | Add `"Year"` and `"Country"` fields    |
| `split_tools_to_individual_files.py` | Split tools into one file per tool     |
| `CategoryPredictor.py`               | Predict `"Tracks"` using GPT/Gemini    |
| `add_github_urls.py`                 | Add `"Github URL"` using Serper API    |
| `flatten_tool_folders.py`            | Flatten nested folders                 |
| `AutoReadme.py`                      | Generate README.md files automatically |
| `run.py`                             | ğŸŸ¢ One-click pipeline to run all steps |

---

